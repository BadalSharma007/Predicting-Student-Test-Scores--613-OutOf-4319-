{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Playground Series S6E1: Student Exam Score Prediction\n",
    "\n",
    "**Approach:** XGBoost with Ridge Regression meta-feature stacking\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "1. Load competition and original datasets\n",
    "2. Engineer 34 numeric features (polynomials, logs, interactions, bins)\n",
    "3. Train Ridge regression with target encoding â†’ generate OOF meta-feature\n",
    "4. Train XGBoost with native categorical support + Ridge predictions as feature\n",
    "5. Generate submission via fold averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Warning: Original dataset not found at /Users/badalkr.sharma/Documents/Kaggle Competitions/Predicting student test score/playground-series-s6e1/Exam_Score_Prediction.csv\n",
      "Continuing without original dataset (this may slightly reduce performance)\n",
      "Train:    (630000, 13)\n",
      "Test:     (270000, 12)\n",
      "\n",
      "Base features: 11\n",
      "Categorical:   ['gender', 'course', 'internet_access', 'sleep_quality', 'study_method', 'facility_rating', 'exam_difficulty']\n",
      "\n",
      "Applying feature engineering...\n",
      "Engineered features: 34\n",
      "Total features:      45 (11 base + 34 engineered)\n",
      "\n",
      "==================================================\n",
      "TRAINING RIDGE REGRESSION\n",
      "==================================================\n",
      "Fold  1 | RMSE: 8.870377\n",
      "Fold  2 | RMSE: 8.962600\n",
      "Fold  3 | RMSE: 8.867122\n",
      "Fold  4 | RMSE: 8.877534\n",
      "Fold  5 | RMSE: 8.889717\n",
      "Fold  6 | RMSE: 8.903042\n",
      "Fold  7 | RMSE: 8.889881\n",
      "Fold  8 | RMSE: 8.867915\n",
      "Fold  9 | RMSE: 8.917761\n",
      "Fold 10 | RMSE: 8.889603\n",
      "\n",
      "Ridge OOF RMSE: 8.893598\n",
      "\n",
      "==================================================\n",
      "PREPARING DATA FOR XGBOOST\n",
      "==================================================\n",
      "Final feature count: 46 (including Ridge meta-feature)\n",
      "\n",
      "==================================================\n",
      "TRAINING XGBOOST\n",
      "==================================================\n",
      "\n",
      "Fold 1/10\n",
      "[0]\tvalidation_0-rmse:18.86022\n",
      "[1000]\tvalidation_0-rmse:8.61629\n",
      "[1947]\tvalidation_0-rmse:8.59355\n",
      "Validation RMSE: 8.59336\n",
      "\n",
      "Fold 2/10\n",
      "[0]\tvalidation_0-rmse:18.90668\n",
      "[1000]\tvalidation_0-rmse:8.70249\n",
      "[2000]\tvalidation_0-rmse:8.67850\n",
      "[2097]\tvalidation_0-rmse:8.67870\n",
      "Validation RMSE: 8.67848\n",
      "\n",
      "Fold 3/10\n",
      "[0]\tvalidation_0-rmse:18.76797\n",
      "[1000]\tvalidation_0-rmse:8.60839\n",
      "[2000]\tvalidation_0-rmse:8.58609\n",
      "[2050]\tvalidation_0-rmse:8.58594\n",
      "Validation RMSE: 8.58582\n",
      "\n",
      "Fold 4/10\n",
      "[0]\tvalidation_0-rmse:18.83416\n",
      "[1000]\tvalidation_0-rmse:8.63504\n",
      "[1939]\tvalidation_0-rmse:8.61343\n",
      "Validation RMSE: 8.61329\n",
      "\n",
      "Fold 5/10\n",
      "[0]\tvalidation_0-rmse:18.93459\n",
      "[1000]\tvalidation_0-rmse:8.63572\n",
      "[2000]\tvalidation_0-rmse:8.61130\n",
      "[2019]\tvalidation_0-rmse:8.61124\n",
      "Validation RMSE: 8.61118\n",
      "\n",
      "Fold 6/10\n",
      "[0]\tvalidation_0-rmse:18.88576\n",
      "[1000]\tvalidation_0-rmse:8.65213\n",
      "[1849]\tvalidation_0-rmse:8.63286\n",
      "Validation RMSE: 8.63257\n",
      "\n",
      "Fold 7/10\n",
      "[0]\tvalidation_0-rmse:18.88079\n",
      "[1000]\tvalidation_0-rmse:8.62589\n",
      "[2000]\tvalidation_0-rmse:8.59775\n",
      "[2133]\tvalidation_0-rmse:8.59771\n",
      "Validation RMSE: 8.59759\n",
      "\n",
      "Fold 8/10\n",
      "[0]\tvalidation_0-rmse:18.82913\n",
      "[1000]\tvalidation_0-rmse:8.60732\n",
      "[1975]\tvalidation_0-rmse:8.58312\n",
      "Validation RMSE: 8.58280\n",
      "\n",
      "Fold 9/10\n",
      "[0]\tvalidation_0-rmse:18.85541\n",
      "[1000]\tvalidation_0-rmse:8.65342\n",
      "[2000]\tvalidation_0-rmse:8.63143\n",
      "[2023]\tvalidation_0-rmse:8.63133\n",
      "Validation RMSE: 8.63130\n",
      "\n",
      "Fold 10/10\n",
      "[0]\tvalidation_0-rmse:18.85261\n",
      "[1000]\tvalidation_0-rmse:8.62891\n",
      "[2000]\tvalidation_0-rmse:8.60529\n",
      "[2023]\tvalidation_0-rmse:8.60523\n",
      "Validation RMSE: 8.60517\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE SUMMARY\n",
      "==================================================\n",
      "Ridge OOF RMSE:   8.893598\n",
      "XGBoost OOF RMSE: 8.61320\n",
      "\n",
      "Feature Summary\n",
      "----------------------------------------\n",
      "Base features:       11\n",
      "Engineered features: 34\n",
      "Meta-feature:        1\n",
      "Total:               46\n",
      "\n",
      "==================================================\n",
      "FILES SAVED\n",
      "==================================================\n",
      "  /Users/badalkr.sharma/Documents/Kaggle Competitions/Predicting student test score/playground-series-s6e1/submission_optimized.csv\n",
      "  /Users/badalkr.sharma/Documents/Kaggle Competitions/Predicting student test score/playground-series-s6e1/xgb_oof_optimized.csv\n",
      "  /Users/badalkr.sharma/Documents/Kaggle Competitions/Predicting student test score/playground-series-s6e1/feature_importance_optimized.csv\n",
      "\n",
      "Top 10 Features by Gain\n",
      "--------------------------------------------------\n",
      "                     feature  importance_pct\n",
      "               study_bin_num       38.572141\n",
      "                  ridge_pred        9.343870\n",
      "study_hours_times_attendance        9.083980\n",
      "            sqrt_study_hours        9.047689\n",
      "             log_study_hours        8.753317\n",
      "         study_hours_squared        6.818366\n",
      "             high_study_flag        2.566555\n",
      "          attendance_bin_num        2.204172\n",
      "           facility_x_sleepq        1.736033\n",
      "     study_hours_times_sleep        1.516691\n",
      "\n",
      "==================================================\n",
      "EXECUTION COMPLETE!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Student Test Score Prediction - XGBoost with Ridge Regression\n",
    "Modified for local execution\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# ============================================================================\n",
    "# FILE PATHS - UPDATE THESE TO MATCH YOUR LOCAL SYSTEM\n",
    "# ============================================================================\n",
    "TRAIN_PATH = \"/Users/badalkr.sharma/Documents/Kaggle Competitions/Predicting student test score/playground-series-s6e1/train.csv\"\n",
    "TEST_PATH = \"/Users/badalkr.sharma/Documents/Kaggle Competitions/Predicting student test score/playground-series-s6e1/test.csv\"\n",
    "SUBMISSION_PATH = \"/Users/badalkr.sharma/Documents/Kaggle Competitions/Predicting student test score/playground-series-s6e1/sample_submission.csv\"\n",
    "\n",
    "# Original dataset path - UPDATE THIS IF YOU HAVE THE ORIGINAL DATASET\n",
    "# If you don't have this file, the code will work without it (see below)\n",
    "ORIGINAL_PATH = \"/Users/badalkr.sharma/Documents/Kaggle Competitions/Predicting student test score/playground-series-s6e1/Exam_Score_Prediction.csv\"\n",
    "\n",
    "# Output directory - files will be saved in the same location as input files\n",
    "OUTPUT_DIR = \"/Users/badalkr.sharma/Documents/Kaggle Competitions/Predicting student test score/playground-series-s6e1/\"\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "TARGET = \"exam_score\"\n",
    "ID_COL = \"id\"\n",
    "N_FOLDS = 10\n",
    "RANDOM_STATE = 1003\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATA\n",
    "# ============================================================================\n",
    "print(\"Loading data...\")\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "submission_df = pd.read_csv(SUBMISSION_PATH)\n",
    "\n",
    "# Try to load original dataset, if not available, continue without it\n",
    "try:\n",
    "    original_df = pd.read_csv(ORIGINAL_PATH)\n",
    "    use_original = True\n",
    "    print(f\"Train:    {train_df.shape}\")\n",
    "    print(f\"Test:     {test_df.shape}\")\n",
    "    print(f\"Original: {original_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Original dataset not found at {ORIGINAL_PATH}\")\n",
    "    print(\"Continuing without original dataset (this may slightly reduce performance)\")\n",
    "    original_df = None\n",
    "    use_original = False\n",
    "    print(f\"Train:    {train_df.shape}\")\n",
    "    print(f\"Test:     {test_df.shape}\")\n",
    "\n",
    "base_features = [c for c in train_df.columns if c not in [TARGET, ID_COL]]\n",
    "cat_features = train_df.select_dtypes(\"object\").columns.tolist()\n",
    "\n",
    "print(f\"\\nBase features: {len(base_features)}\")\n",
    "print(f\"Categorical:   {cat_features}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "def engineer_features(df, base_cols):\n",
    "    \"\"\"Generate engineered features from raw data.\"\"\"\n",
    "    out = df.copy()\n",
    "    eps = 1e-5\n",
    "    \n",
    "    # Clipped values for safe transforms\n",
    "    study = out['study_hours'].clip(lower=0)\n",
    "    attend = out['class_attendance'].clip(lower=0)\n",
    "    sleep = out['sleep_hours'].clip(lower=0)\n",
    "    \n",
    "    # Polynomial features\n",
    "    out['study_hours_squared'] = out['study_hours'] ** 2\n",
    "    out['class_attendance_squared'] = out['class_attendance'] ** 2\n",
    "    out['sleep_hours_squared'] = out['sleep_hours'] ** 2\n",
    "    out['age_squared'] = out['age'] ** 2\n",
    "    \n",
    "    # Log transforms\n",
    "    out['log_study_hours'] = np.log1p(study)\n",
    "    out['log_class_attendance'] = np.log1p(attend)\n",
    "    out['log_sleep_hours'] = np.log1p(sleep)\n",
    "    \n",
    "    # Sqrt transforms\n",
    "    out['sqrt_study_hours'] = np.sqrt(study)\n",
    "    out['sqrt_class_attendance'] = np.sqrt(attend)\n",
    "    \n",
    "    # Interactions\n",
    "    out['study_hours_times_attendance'] = out['study_hours'] * out['class_attendance']\n",
    "    out['study_hours_times_sleep'] = out['study_hours'] * out['sleep_hours']\n",
    "    out['attendance_times_sleep'] = out['class_attendance'] * out['sleep_hours']\n",
    "    out['age_times_study_hours'] = out['age'] * out['study_hours']\n",
    "    \n",
    "    # Ratios\n",
    "    out['study_hours_over_sleep'] = out['study_hours'] / (out['sleep_hours'] + eps)\n",
    "    out['attendance_over_sleep'] = out['class_attendance'] / (out['sleep_hours'] + eps)\n",
    "    out['attendance_over_study'] = out['class_attendance'] / (out['study_hours'] + eps)\n",
    "    \n",
    "    # Ordinal mappings\n",
    "    ordinal_maps = {\n",
    "        'sleep_quality': {'poor': 0, 'average': 1, 'good': 2},\n",
    "        'facility_rating': {'low': 0, 'medium': 1, 'high': 2},\n",
    "        'exam_difficulty': {'easy': 0, 'moderate': 1, 'hard': 2}\n",
    "    }\n",
    "    for col, mapping in ordinal_maps.items():\n",
    "        out[f'{col}_numeric'] = out[col].map(mapping).fillna(1).astype(int)\n",
    "    \n",
    "    # Ordinal interactions\n",
    "    out['study_hours_times_sleep_quality'] = out['study_hours'] * out['sleep_quality_numeric']\n",
    "    out['attendance_times_facility'] = out['class_attendance'] * out['facility_rating_numeric']\n",
    "    out['sleep_hours_times_difficulty'] = out['sleep_hours'] * out['exam_difficulty_numeric']\n",
    "    out['facility_x_sleepq'] = out['facility_rating_numeric'] * out['sleep_quality_numeric']\n",
    "    out['difficulty_x_facility'] = out['exam_difficulty_numeric'] * out['facility_rating_numeric']\n",
    "    \n",
    "    # Rule-based flags\n",
    "    out['high_att_high_study'] = ((out['class_attendance'] >= 90) & (out['study_hours'] >= 6)).astype(int)\n",
    "    out['ideal_sleep_flag'] = ((out['sleep_hours'] >= 7) & (out['sleep_hours'] <= 9)).astype(int)\n",
    "    out['high_study_flag'] = (out['study_hours'] >= 7).astype(int)\n",
    "    \n",
    "    # Composite efficiency\n",
    "    out['efficiency'] = (out['study_hours'] * out['class_attendance']) / (out['sleep_hours'] + 1)\n",
    "    \n",
    "    # Gap features\n",
    "    out['sleep_gap_8'] = (out['sleep_hours'] - 8.0).abs()\n",
    "    out['attendance_gap_100'] = (out['class_attendance'] - 100.0).abs()\n",
    "    \n",
    "    # Binned features\n",
    "    out['study_bin_num'] = pd.cut(out['study_hours'], bins=5, labels=False).astype(int)\n",
    "    out['attendance_bin_num'] = pd.cut(out['class_attendance'], bins=5, labels=False).astype(int)\n",
    "    out['sleep_bin_num'] = pd.cut(out['sleep_hours'], bins=5, labels=False).astype(int)\n",
    "    out['age_bin_num'] = pd.cut(out['age'], bins=5, labels=False).astype(int)\n",
    "    \n",
    "    engineered_cols = [\n",
    "        'study_hours_squared', 'class_attendance_squared', 'sleep_hours_squared', 'age_squared',\n",
    "        'log_study_hours', 'log_class_attendance', 'log_sleep_hours',\n",
    "        'sqrt_study_hours', 'sqrt_class_attendance',\n",
    "        'study_hours_times_attendance', 'study_hours_times_sleep', 'attendance_times_sleep',\n",
    "        'age_times_study_hours',\n",
    "        'study_hours_over_sleep', 'attendance_over_sleep', 'attendance_over_study',\n",
    "        'sleep_quality_numeric', 'facility_rating_numeric', 'exam_difficulty_numeric',\n",
    "        'study_hours_times_sleep_quality', 'attendance_times_facility', 'sleep_hours_times_difficulty',\n",
    "        'facility_x_sleepq', 'difficulty_x_facility',\n",
    "        'high_att_high_study', 'ideal_sleep_flag', 'high_study_flag',\n",
    "        'efficiency',\n",
    "        'sleep_gap_8', 'attendance_gap_100',\n",
    "        'study_bin_num', 'attendance_bin_num', 'sleep_bin_num', 'age_bin_num'\n",
    "    ]\n",
    "    \n",
    "    return out[base_cols + engineered_cols], engineered_cols\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\"\\nApplying feature engineering...\")\n",
    "X_train, engineered_cols = engineer_features(train_df, base_features)\n",
    "X_test, _ = engineer_features(test_df, base_features)\n",
    "\n",
    "y_train = train_df[TARGET].reset_index(drop=True)\n",
    "\n",
    "# Handle original data if available\n",
    "if use_original:\n",
    "    X_orig, _ = engineer_features(original_df, base_features)\n",
    "    y_orig = original_df[TARGET].reset_index(drop=True)\n",
    "    full_data = pd.concat([X_train, X_test, X_orig], axis=0, ignore_index=True)\n",
    "else:\n",
    "    X_orig = None\n",
    "    y_orig = None\n",
    "    full_data = pd.concat([X_train, X_test], axis=0, ignore_index=True)\n",
    "\n",
    "for col in engineered_cols:\n",
    "    full_data[col] = full_data[col].astype(float)\n",
    "\n",
    "n_train, n_test = len(train_df), len(test_df)\n",
    "X = full_data.iloc[:n_train].copy()\n",
    "X_test = full_data.iloc[n_train:n_train + n_test].copy()\n",
    "\n",
    "if use_original:\n",
    "    X_original = full_data.iloc[n_train + n_test:].copy()\n",
    "else:\n",
    "    X_original = None\n",
    "\n",
    "print(f\"Engineered features: {len(engineered_cols)}\")\n",
    "print(f\"Total features:      {X.shape[1]} (11 base + {len(engineered_cols)} engineered)\")\n",
    "\n",
    "# ============================================================================\n",
    "# RIDGE REGRESSION WITH CROSS-VALIDATION\n",
    "# ============================================================================\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "oof_ridge = np.zeros(len(X))\n",
    "test_preds_ridge = np.zeros((len(X_test), N_FOLDS))\n",
    "if use_original:\n",
    "    orig_preds_ridge = np.zeros(len(X_original))\n",
    "\n",
    "ridge_alphas = np.logspace(-3, 3, 20)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING RIDGE REGRESSION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y_train), 1):\n",
    "    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    # Augment with original data if available\n",
    "    if use_original:\n",
    "        X_tr_aug = pd.concat([X_tr, X_original], axis=0)\n",
    "        y_tr_aug = pd.concat([y_tr, y_orig], axis=0)\n",
    "    else:\n",
    "        X_tr_aug = X_tr\n",
    "        y_tr_aug = y_tr\n",
    "    \n",
    "    # Target encode categoricals\n",
    "    encoder = TargetEncoder(smooth='auto', target_type='continuous')\n",
    "    X_tr_enc = X_tr_aug.copy()\n",
    "    X_val_enc = X_val.copy()\n",
    "    X_test_enc = X_test.copy()\n",
    "    \n",
    "    X_tr_enc[cat_features] = encoder.fit_transform(X_tr_aug[cat_features], y_tr_aug)\n",
    "    X_val_enc[cat_features] = encoder.transform(X_val[cat_features])\n",
    "    X_test_enc[cat_features] = encoder.transform(X_test[cat_features])\n",
    "    \n",
    "    # Fit Ridge\n",
    "    ridge = RidgeCV(alphas=ridge_alphas, cv=5, scoring='neg_root_mean_squared_error')\n",
    "    ridge.fit(X_tr_enc, y_tr_aug.values.ravel())\n",
    "    \n",
    "    # Predictions (clipped to valid range)\n",
    "    oof_ridge[val_idx] = np.clip(ridge.predict(X_val_enc), 0, 100)\n",
    "    test_preds_ridge[:, fold - 1] = np.clip(ridge.predict(X_test_enc), 0, 100)\n",
    "    \n",
    "    if use_original:\n",
    "        orig_preds_ridge += np.clip(ridge.predict(X_tr_enc.iloc[-len(X_original):]), 0, 100) / N_FOLDS\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_val, oof_ridge[val_idx]))\n",
    "    print(f\"Fold {fold:2d} | RMSE: {rmse:.6f}\")\n",
    "\n",
    "ridge_oof_rmse = np.sqrt(mean_squared_error(y_train, oof_ridge))\n",
    "print(f\"\\nRidge OOF RMSE: {ridge_oof_rmse:.6f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# XGBOOST WITH RIDGE META-FEATURE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREPARING DATA FOR XGBOOST\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Convert categoricals for XGBoost native handling\n",
    "if use_original:\n",
    "    full_data = pd.concat([X_train, X_test, X_orig], axis=0, ignore_index=True)\n",
    "else:\n",
    "    full_data = pd.concat([X_train, X_test], axis=0, ignore_index=True)\n",
    "\n",
    "for col in base_features:\n",
    "    full_data[col] = full_data[col].astype(str).astype(\"category\")\n",
    "for col in engineered_cols:\n",
    "    full_data[col] = full_data[col].astype(float)\n",
    "\n",
    "X_xgb = full_data.iloc[:n_train].copy()\n",
    "X_test_xgb = full_data.iloc[n_train:n_train + n_test].copy()\n",
    "\n",
    "if use_original:\n",
    "    X_orig_xgb = full_data.iloc[n_train + n_test:].copy()\n",
    "else:\n",
    "    X_orig_xgb = None\n",
    "\n",
    "# Add Ridge meta-feature\n",
    "X_xgb['ridge_pred'] = oof_ridge\n",
    "X_test_xgb['ridge_pred'] = test_preds_ridge.mean(axis=1)\n",
    "if use_original:\n",
    "    X_orig_xgb['ridge_pred'] = orig_preds_ridge\n",
    "\n",
    "print(f\"Final feature count: {X_xgb.shape[1]} (including Ridge meta-feature)\")\n",
    "\n",
    "# XGBoost parameters\n",
    "# Note: Changed device to 'cpu' for compatibility. Change to 'cuda' if you have GPU\n",
    "xgb_params = {\n",
    "    \"n_estimators\": 20000,\n",
    "    \"learning_rate\": 0.004,\n",
    "    \"max_depth\": 9,\n",
    "    \"subsample\": 0.78,\n",
    "    \"colsample_bytree\": 0.55,\n",
    "    \"colsample_bynode\": 0.65,\n",
    "    \"reg_lambda\": 6,\n",
    "    \"reg_alpha\": 0.15,\n",
    "    \"min_child_weight\": 6,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"enable_categorical\": True,\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"early_stopping_rounds\": 100,\n",
    "    \"random_state\": 42,\n",
    "    \"device\": \"cpu\"  # Change to \"cuda\" if you have a GPU\n",
    "}\n",
    "\n",
    "test_preds_xgb = []\n",
    "oof_xgb = np.zeros(len(X_xgb))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING XGBOOST\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_xgb, y_train), 1):\n",
    "    print(f\"\\nFold {fold}/{N_FOLDS}\")\n",
    "    \n",
    "    X_tr, X_val = X_xgb.iloc[train_idx], X_xgb.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    # Augment with original data if available\n",
    "    if use_original:\n",
    "        X_tr_aug = pd.concat([X_tr, X_orig_xgb], axis=0)\n",
    "        y_tr_aug = pd.concat([y_tr, y_orig], axis=0)\n",
    "    else:\n",
    "        X_tr_aug = X_tr\n",
    "        y_tr_aug = y_tr\n",
    "    \n",
    "    model = xgb.XGBRegressor(**xgb_params)\n",
    "    model.fit(\n",
    "        X_tr_aug, y_tr_aug,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=1000\n",
    "    )\n",
    "    \n",
    "    oof_xgb[val_idx] = model.predict(X_val)\n",
    "    test_preds_xgb.append(model.predict(X_test_xgb))\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_val, oof_xgb[val_idx]))\n",
    "    print(f\"Validation RMSE: {rmse:.5f}\")\n",
    "\n",
    "xgb_oof_rmse = np.sqrt(mean_squared_error(y_train, oof_xgb))\n",
    "\n",
    "# ============================================================================\n",
    "# RESULTS SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Ridge OOF RMSE:   {ridge_oof_rmse:.6f}\")\n",
    "print(f\"XGBoost OOF RMSE: {xgb_oof_rmse:.5f}\")\n",
    "\n",
    "print(f\"\\nFeature Summary\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Base features:       {len(base_features)}\")\n",
    "print(f\"Engineered features: {len(engineered_cols)}\")\n",
    "print(f\"Meta-feature:        1\")\n",
    "print(f\"Total:               {X_xgb.shape[1]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "import os\n",
    "\n",
    "# OOF predictions\n",
    "oof_df = pd.DataFrame({ID_COL: train_df[ID_COL], TARGET: oof_xgb})\n",
    "oof_path = os.path.join(OUTPUT_DIR, \"xgb_oof_optimized.csv\")\n",
    "oof_df.to_csv(oof_path, index=False)\n",
    "\n",
    "# Submission\n",
    "submission = submission_df.copy()\n",
    "submission[TARGET] = np.mean(test_preds_xgb, axis=0)\n",
    "submission_path = os.path.join(OUTPUT_DIR, \"submission_optimized.csv\")\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "# Feature importance\n",
    "importance_scores = model.get_booster().get_score(importance_type=\"gain\")\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    \"feature\": list(importance_scores.keys()),\n",
    "    \"importance\": list(importance_scores.values())\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "importance_df['importance_pct'] = 100 * importance_df['importance'] / importance_df['importance'].sum()\n",
    "importance_path = os.path.join(OUTPUT_DIR, \"feature_importance_optimized.csv\")\n",
    "importance_df.to_csv(importance_path, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FILES SAVED\")\n",
    "print(\"=\"*50)\n",
    "print(f\"  {submission_path}\")\n",
    "print(f\"  {oof_path}\")\n",
    "print(f\"  {importance_path}\")\n",
    "\n",
    "print(\"\\nTop 10 Features by Gain\")\n",
    "print(\"-\" * 50)\n",
    "print(importance_df.head(10)[['feature', 'importance_pct']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXECUTION COMPLETE!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14993753,
     "sourceId": 119082,
     "sourceType": "competition"
    },
    {
     "datasetId": 8762382,
     "sourceId": 13904981,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
